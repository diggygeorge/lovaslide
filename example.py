#!/usr/bin/env python3
"""
Example usage of LovaSlide
"""

from backend.analyzer import Analyzer

def main():
    # Sample text about remote work
    sample_text = """
    Provided proper attribution is provided, Google hereby grants permission toreproduce the tables and figures in this paper solely for use in journalistic orscholarly works.Attention Is All You NeedAshish Vaswani∗Google Brainavaswani@google.comNoam Shazeer∗Google Brainnoam@google.comNiki Parmar∗Google Researchnikip@google.comJakob Uszkoreit∗Google Researchusz@google.comLlion Jones∗Google Researchllion@google.comAidan N. Gomez∗†University of Torontoaidan@cs.toronto.eduŁukasz Kaiser∗Google Brainlukaszkaiser@google.comIllia Polosukhin∗‡illia.polosukhin@gmail.comAbstractThe dominant sequence transduction models are based on complex recurrent orconvolutional neural networks that include an encoder and a decoder. The bestperforming models also connect the encoder and decoder through an attentionmechanism. We propose a new simple network architecture, the Transformer,based solely on attention mechanisms, dispensing with recurrence and convolutionsentirely. Experiments on two machine translation tasks show these models tobe superior in quality while being more parallelizable and requiring significantlyless time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, includingensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.8 aftertraining for 3.5 days on eight GPUs, a small fraction of the training costs of thebest models from the literature. We show that the Transformer generalizes well toother tasks by applying it successfully to English constituency parsing both withlarge and limited training data.∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and startedthe effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models andhas been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-headattention and the parameter-free position representation and became the other person involved in nearly everydetail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase andtensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, andefficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of andimplementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively acceleratingour research.†Work performed while at Google Brain.‡Work performed while at Google Research.31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.arXiv:1706.03762v7  [cs.CL]  2 Aug 20231IntroductionRecurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networksin particular, have been firmly established as state of the art approaches in sequence modeling andtransduction problems such as language modeling and machine translation [35, 2, 5]. Numerousefforts have since continued to push the boundaries of recurrent language models and encoder-decoderarchitectures [38, 24, 15].Recurrent models typically factor computation along the symbol positions of the input and outputsequences. Aligning the positions to steps in computation time, they generate a sequence of hiddenstates ht, as a function of the previous hidden state ht−1 and the input for position t. This inherentlysequential nature precludes parallelization within training examples, which becomes critical at longersequence lengths, as memory constraints limit batching across examples. Recent work has achievedsignificant improvements in computational efficiency through factorization tricks [21] and conditionalcomputation [32], while also improving model performance in case of the latter. The fundamentalconstraint of sequential computation, however, remains.Attention mechanisms have become an integral part of compelling sequence modeling and transduc-tion models in various tasks, allowing modeling of dependencies without regard to their distance inthe input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanismsare used in conjunction with a recurrent network.In this work we propose the Transformer, a model architecture eschewing recurrence and insteadrelying entirely on an attention mechanism to draw global dependencies between input and output.The Transformer allows for significantly more parallelization and can reach a new state of the art intranslation quality after being trained for as little as twelve hours on eight P100 GPUs.2BackgroundThe goal of reducing sequential computation also forms the foundation of the Extended Neural GPU[16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic buildingblock, computing hidden representations in parallel for all input and output positions. In these models,the number of operations required to relate signals from two arbitrary input or output positions growsin the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makesit more difficult to learn dependencies between distant positions [12]. In the Transformer this isreduced to a constant number of operations, albeit at the cost of reduced effective resolution dueto averaging attention-weighted positions, an effect we counteract with Multi-Head Attention asdescribed in section 3.2.Self-attention, sometimes called intra-attention is an attention mechanism relating different positionsof a single sequence in order to compute a representation of the sequence. Self-attention has beenused successfully in a variety of tasks including reading comprehension, abstractive summarization,textual entailment and learning task-independent sentence representations [4, 27, 28, 22].End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-aligned recurrence and have been shown to perform well on simple-language question answering andlanguage modeling tasks [34].To the best of our knowledge, however, the Transformer is the first transduction model relyingentirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution. In the following sections, we will describe the Transformer, motivateself-attention and discuss its advantages over models such as [17, 18] and [9].3Model ArchitectureMost competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35].Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequenceof continuous representations z = (z1, ..., zn). Given z, the decoder then generates an outputsequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive[10], consuming the previously generated symbols as additional input when generating the next.2Figure 1: The Transformer - model architecture.The Transformer follows this overall architecture using stacked self-attention and point-wise, fullyconnected layers for both the encoder and decoder, shown in the left and right halves of Figure 1,respectively.3.1Encoder and Decoder StacksEncoder:The encoder is composed of a stack of N = 6 identical layers. Each layer has twosub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network. We employ a residual connection [11] around each ofthe two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer isLayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layeritself. To facilitate these residual connections, all sub-layers in the model, as well as the embeddinglayers, produce outputs of dimension dmodel = 512.Decoder:The decoder is also composed of a stack of N = 6 identical layers. In addition to the twosub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-headattention over the output of the encoder stack. Similar to the encoder, we employ residual connectionsaround each of the sub-layers, followed by layer normalization. We also modify the self-attentionsub-layer in the decoder stack to prevent positions from attending to subsequent positions. Thismasking, combined with fact that the output embeddings are offset by one position, ensures that thepredictions for position i can depend only on the known outputs at positions less than i.3.2AttentionAn attention function can be described as mapping a query and a set of key-value pairs to an output,where the query, keys, values, and output are all vectors. The output is computed as a weighted sum3Scaled Dot-Product AttentionMulti-Head AttentionFigure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of severalattention layers running in parallel.of the values, where the weight assigned to each value is computed by a compatibility function of thequery with the corresponding key.3.2.1Scaled Dot-Product AttentionWe call our particular attention "Scaled Dot-Product Attention" (Figure 2). The input consists ofqueries and keys of dimension dk, and values of dimension dv. We compute the dot products of thequery with all keys, divide each by √dk, and apply a softmax function to obtain the weights on thevalues.In practice, we compute the attention function on a set of queries simultaneously, packed togetherinto a matrix Q. The keys and values are also packed together into matrices K and V . We computethe matrix of outputs as:Attention(Q, K, V ) = softmax(QKT√dk)V(1)The two most commonly used attention functions are additive attention [2], and dot-product (multi-plicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factorof1√dk . Additive attention computes the compatibility function using a feed-forward network witha single hidden layer. While the two are similar in theoretical complexity, dot-product attention ismuch faster and more space-efficient in practice, since it can be implemented using highly optimizedmatrix multiplication code.While for small values of dk the two mechanisms perform similarly, additive attention outperformsdot product attention without scaling for larger values of dk [3]. We suspect that for large values ofdk, the dot products grow large in magnitude, pushing the softmax function into regions where it hasextremely small gradients 4. To counteract this effect, we scale the dot products by1√dk .3.2.2Multi-Head AttentionInstead of performing a single attention function with dmodel-dimensional keys, values and queries,we found it beneficial to linearly project the queries, keys and values h times with different, learnedlinear projections to dk, dk and dv dimensions, respectively. On each of these projected versions ofqueries, keys and values we then perform the attention function in parallel, yielding dv-dimensional4To illustrate why the dot products get large, assume that the components of q and k are independent randomvariables with mean 0 and variance 1. Then their dot product, q · k = Pdki=1 qiki, has mean 0 and variance dk.4output values. These are concatenated and once again projected, resulting in the final values, asdepicted in Figure 2.Multi-head attention allows the model to jointly attend to information from different representationsubspaces at different positions. With a single attention head, averaging inhibits this.MultiHead(Q, K, V ) = Concat(head1, ..., headh)W Owhere headi = Attention(QW Qi , KW Ki , V W Vi )Where the projections are parameter matrices W Qi∈Rdmodel×dk, W Ki∈Rdmodel×dk, W Vi∈Rdmodel×dvand W O ∈Rhdv×dmodel.In this work we employ h = 8 parallel attention layers, or heads. For each of these we usedk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational costis similar to that of single-head attention with full dimensionality.3.2.3Applications of Attention in our ModelThe Transformer uses multi-head attention in three different ways:• In "encoder-decoder attention" layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder. This allows everyposition in the decoder to attend over all positions in the input sequence. This mimics thetypical encoder-decoder attention mechanisms in sequence-to-sequence models such as[38, 2, 9].• The encoder contains self-attention layers. In a self-attention layer all of the keys, valuesand queries come from the same place, in this case, the output of the previous layer in theencoder. Each position in the encoder can attend to all positions in the previous layer of theencoder.• Similarly, self-attention layers in the decoder allow each position in the decoder to attend toall positions in the decoder up to and including that position. We need to prevent leftwardinformation flow in the decoder to preserve the auto-regressive property. We implement thisinside of scaled dot-product attention by masking out (setting to −∞)
    """
    
    try:
        # Initialize Analyzer
        print("🚀 Initializing Analyzer...")
        analyzer = Analyzer()
        
        # Create presentation
        print("📝 Creating presentation from text...")
        presentation = analyzer.create_slides(sample_text, max_slides=5)
        
        # Print to console
        analyzer.print_slides(presentation)
        
        # Export slides data to JSON
        print("\n💾 Exporting slides data...")
        analyzer.export_slides_data(presentation, "backend/data/remote_work_slides_data.json")
        
        print("\n✅ Done! Check the exported file:")
        print("  🎯 backend/data/remote_work_slides_data.json - Slides data (titles & bullets)")
        
    except ValueError as e:
        print(f"❌ Error: {e}")
        print("\nPlease set your OpenAI API key as an environment variable:")
        print("export OPENAI_API_KEY='your-api-key-here'")
        print("\nOr create a .env file with:")
        print("OPENAI_API_KEY=your-api-key-here")
    except Exception as e:
        print(f"❌ An error occurred: {e}")

if __name__ == "__main__":
    main()
